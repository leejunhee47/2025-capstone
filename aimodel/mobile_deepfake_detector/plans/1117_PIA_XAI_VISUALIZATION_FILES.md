# PIA XAI Visualization Files Tree

PIA (Phoneme-Identity-Appearance) ëª¨ë¸ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ - ì „ì²˜ë¦¬ë¶€í„° í•™ìŠµ, XAI ì‹œê°í™”ê¹Œì§€

**ìƒì„±ì¼**: 2025-11-17
**ì°¸ì¡° ë³´ê³ ì„œ**:
- `diary/2025-11-14_MAR_ìˆ˜ì •_ì—°êµ¬ë³´ê³ ì„œ.md`
- `diary/2025-11-15_PIA_XAI_ê²°ê³¼_ë¹„êµë¶„ì„.md`
- `diary/2025-11-15_XAI_êµ¬í˜„_ë°_ëª¨ë¸_ë¶„ì„.md`

---

## ğŸ“ Complete PIA Pipeline Tree

```
E:\capstone\
â”œâ”€â”€ ğŸ“‚ ì „ì²˜ë¦¬ ë°ì´í„° (Preprocessed Data)
â”‚   â”œâ”€â”€ preprocessed_data_phoneme/          # âœ… PIA ì „ìš© ì „ì²˜ë¦¬ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ train/                          # í•™ìŠµ ë°ì´í„° (NPZ íŒŒì¼ë“¤)
â”‚   â”‚   â”‚   â”œâ”€â”€ 00000.npz                   # frames, audio, lip, arcface, geometry(MAR), phoneme_labels, timestamps
â”‚   â”‚   â”‚   â”œâ”€â”€ 00001.npz
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ val/                            # ê²€ì¦ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ test/                           # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ train_index.json               # í•™ìŠµ ë°ì´í„° ì¸ë±ìŠ¤
â”‚   â”‚   â”œâ”€â”€ val_index.json                 # ê²€ì¦ ë°ì´í„° ì¸ë±ìŠ¤
â”‚   â”‚   â””â”€â”€ test_index.json                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ë±ìŠ¤
â”‚   â”‚
â”‚   â””â”€â”€ preprocessed_data_real/             # âš ï¸ êµ¬ë²„ì „ (ìŒì†Œ ë¼ë²¨ ì—†ìŒ)
â”‚       â””â”€â”€ train/test/val/
â”‚
â”œâ”€â”€ ğŸ“‚ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (Preprocessing Scripts)
â”‚   â”œâ”€â”€ preprocess_parallel.py              # âœ… í•µì‹¬ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”‚   â””â”€â”€ ê¸°ëŠ¥:
â”‚   â”‚       â”œâ”€â”€ HybridPhonemeAlignerë¡œ ìŒì†Œ ì¶”ì¶œ
â”‚   â”‚       â”œâ”€â”€ EnhancedMARExtractorë¡œ MAR ì¶”ì¶œ
â”‚   â”‚       â”œâ”€â”€ ArcFaceExtractorë¡œ ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ
â”‚   â”‚       â”œâ”€â”€ ë©€í‹°í”„ë¡œì„¸ì‹± ë³‘ë ¬ ì²˜ë¦¬
â”‚   â”‚       â””â”€â”€ ì¶œë ¥: preprocessed_data_phoneme/
â”‚   â”‚
â”‚   â”œâ”€â”€ test_phoneme_preprocessing.py
â”‚   â”œâ”€â”€ test_single_video_phoneme.py
â”‚   â””â”€â”€ verify_phoneme_accuracy.py
â”‚
â”œâ”€â”€ ğŸ“‚ ëª¨ë°”ì¼ ë”¥í˜ì´í¬ íƒì§€ê¸° (Mobile Deepfake Detector)
â”‚   â””â”€â”€ mobile_deepfake_detector/
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ configs/                     # ì„¤ì • íŒŒì¼
â”‚       â”‚   â”œâ”€â”€ train_pia.yaml              # âœ… PIA í•™ìŠµ ì„¤ì •
â”‚       â”‚   â”‚   â””â”€â”€ ë‚´ìš©: num_phonemes=14, frames_per_phoneme=5, arcface_dim=512, geo_dim=1
â”‚       â”‚   â”œâ”€â”€ train_teacher_korean.yaml   # MMMS-BA í•™ìŠµ ì„¤ì •
â”‚       â”‚   â””â”€â”€ phoneme_vocab.json          # 14ê°œ í•µì‹¬ í•œêµ­ì–´ ìŒì†Œ vocabulary
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ src/                         # ì†ŒìŠ¤ ì½”ë“œ
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ ğŸ“‚ data/                    # ë°ì´í„° ë¡œë”©
â”‚       â”‚   â”‚   â”œâ”€â”€ dataset.py              # MMMS-BA Dataset
â”‚       â”‚   â”‚   â”œâ”€â”€ phoneme_dataset.py      # âœ… PIA Dataset (KoreanPhonemeDataset)
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ __getitem__():
â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ NPZ íŒŒì¼ ë¡œë“œ
â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ ìŒì†Œ â†’ 14Ã—5 ê·¸ë¦¬ë“œ ë§¤ì¹­
â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ MAR, ArcFace, Frames ì¶”ì¶œ
â”‚       â”‚   â”‚   â”‚       â””â”€â”€ ì¶œë ¥: {geometry, images, arcface, mask, phonemes, label}
â”‚       â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # ShortsPreprocessor (ë¹„ë””ì˜¤â†’NPZ)
â”‚       â”‚   â”‚   â””â”€â”€ korean_phoneme_vocab.py # ìŒì†Œ vocabulary ê´€ë¦¬
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ ğŸ“‚ models/                  # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚       â”‚   â”‚   â”œâ”€â”€ teacher.py              # MMMS-BA ëª¨ë¸ (Tri-modal)
â”‚       â”‚   â”‚   â””â”€â”€ pia_model.py            # âœ… PIA ëª¨ë¸ (Tri-branch)
â”‚       â”‚   â”‚       â””â”€â”€ class PIAModel:
â”‚       â”‚   â”‚           â”œâ”€â”€ GeometryBranch (MAR â†’ GRU)
â”‚       â”‚   â”‚           â”œâ”€â”€ ImageBranch (Frames â†’ ResNet â†’ GRU)
â”‚       â”‚   â”‚           â”œâ”€â”€ ArcBranch (ArcFace â†’ GRU)
â”‚       â”‚   â”‚           â”œâ”€â”€ CrossAttention (3-branch fusion)
â”‚       â”‚   â”‚           â””â”€â”€ forward() â†’ (logits, branch_outputs)
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ ğŸ“‚ utils/                   # ìœ í‹¸ë¦¬í‹°
â”‚       â”‚   â”‚   â”‚
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ¤ ìŒì†Œ ì¶”ì¶œ (Phoneme Extraction)
â”‚       â”‚   â”‚   â”œâ”€â”€ hybrid_phoneme_aligner_v2.py  # âœ… ì‹¤ì œ ì‚¬ìš© (WhisperX + Wav2Vec2)
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ class HybridPhonemeAligner:
â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ align_video(video_path) â†’ {phonemes, intervals, transcription}
â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ _align_segment_pia_style()  # 3ë‹¨ê³„: ê· ë“±â†’WhisperXâ†’ìëª¨ ë¶„ë°°
â”‚       â”‚   â”‚   â”‚       â””â”€â”€ _extract_and_distribute_chars()  # ìŒì ˆâ†’ìëª¨ ë¶„í•´
â”‚       â”‚   â”‚   â”‚
â”‚       â”‚   â”‚   â”œâ”€â”€ wav2vec2_korean_phoneme_aligner.py  # âš ï¸ ì‚¬ìš© ì•ˆ í•¨ (êµ¬ë²„ì „)
â”‚       â”‚   â”‚   â”œâ”€â”€ hybrid_phoneme_aligner.py           # âš ï¸ ì‚¬ìš© ì•ˆ í•¨ (v1)
â”‚       â”‚   â”‚   â”œâ”€â”€ hybrid_phoneme_aligner_v3_failed.py # âš ï¸ ì‹¤íŒ¨ ë²„ì „
â”‚       â”‚   â”‚   â”œâ”€â”€ pia_main_phoneme_aligner.py
â”‚       â”‚   â”‚   â”œâ”€â”€ phoneme_classifier.py
â”‚       â”‚   â”‚   â”œâ”€â”€ phoneme_filter.py
â”‚       â”‚   â”‚   â”œâ”€â”€ phoneme_mar_matcher.py
â”‚       â”‚   â”‚   â””â”€â”€ korean_phoneme_config.py      # KEEP_PHONEMES_KOREAN (14ê°œ)
â”‚       â”‚   â”‚   â”‚
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ‘„ ê¸°í•˜í•™ íŠ¹ì§• ì¶”ì¶œ (Geometry Feature)
â”‚       â”‚   â”‚   â”œâ”€â”€ enhanced_mar_extractor.py     # âœ… MAR ì¶”ì¶œ (v3.2)
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ class EnhancedMARExtractor:
â”‚       â”‚   â”‚   â”‚       â”œâ”€â”€ extract_from_video() â†’ {mar_vertical, mar_horizontal, ...}
â”‚       â”‚   â”‚   â”‚       â””â”€â”€ _calculate_multi_features_relative()  # Face-height ì •ê·œí™”
â”‚       â”‚   â”‚   â”‚
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ˜€ ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œ (Identity Feature)
â”‚       â”‚   â”‚   â”œâ”€â”€ arcface_extractor.py          # âœ… ArcFace ì¶”ì¶œ (buffalo_l)
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ class ArcFaceExtractor:
â”‚       â”‚   â”‚   â”‚       â””â”€â”€ extract_from_video() â†’ (T, 512)
â”‚       â”‚   â”‚   â”‚
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ”§ ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
â”‚       â”‚   â”‚   â”œâ”€â”€ config.py                     # YAML ì„¤ì • ë¡œë”
â”‚       â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚       â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚       â”‚   â”‚   â””â”€â”€ mmms_ba_adapter.py            # MMMS-BA ì–´ëŒ‘í„°
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ ğŸ“‚ xai/                     # âœ… XAI ëª¨ë“ˆ
â”‚       â”‚       â”œâ”€â”€ pia_explainer.py              # âœ… PIA XAI ë¶„ì„ ì—”ì§„ (640 lines)
â”‚       â”‚       â”‚   â””â”€â”€ class PIAExplainer:
â”‚       â”‚       â”‚       â”œâ”€â”€ explain(geoms, imgs, arcs, mask, phonemes, timestamps)
â”‚       â”‚       â”‚       â”‚   â””â”€â”€ ì¶œë ¥: {
â”‚       â”‚       â”‚       â”‚       'prediction': 'FAKE'/'REAL',
â”‚       â”‚       â”‚       â”‚       'confidence': 1.00,
â”‚       â”‚       â”‚       â”‚       'branch_contributions': {'geometry': 15%, 'image': 78%, 'arcface': 7%},
â”‚       â”‚       â”‚       â”‚       'top_branch': 'image',
â”‚       â”‚       â”‚       â”‚       'phoneme_attention': (14, 5) attention weights,
â”‚       â”‚       â”‚       â”‚       'temporal_analysis': {...},
â”‚       â”‚       â”‚       â”‚       'geometry_analysis': {...},
â”‚       â”‚       â”‚       â”‚       'korean_summary': "..."
â”‚       â”‚       â”‚       â”‚   }
â”‚       â”‚       â”‚       â”œâ”€â”€ _compute_branch_contributions()
â”‚       â”‚       â”‚       â”œâ”€â”€ _compute_phoneme_attention()
â”‚       â”‚       â”‚       â”œâ”€â”€ _analyze_temporal_patterns()
â”‚       â”‚       â”‚       â”œâ”€â”€ _analyze_geometry_anomalies()
â”‚       â”‚       â”‚       â””â”€â”€ _generate_korean_explanation()
â”‚       â”‚       â”‚
â”‚       â”‚       â”œâ”€â”€ pia_visualizer.py             # âœ… PIA XAI ì‹œê°í™” (664 lines)
â”‚       â”‚       â”‚   â””â”€â”€ class PIAVisualizer:
â”‚       â”‚       â”‚       â”œâ”€â”€ visualize_full_analysis(xai_result, output_path, video_id)
â”‚       â”‚       â”‚       â”‚   â””â”€â”€ ì¶œë ¥: 4-subplot ê·¸ë˜í”„ PNG
â”‚       â”‚       â”‚       â”‚       â”œâ”€â”€ Branch Contribution (Bar chart)
â”‚       â”‚       â”‚       â”‚       â”œâ”€â”€ Phoneme Attention Heatmap (14Ã—5)
â”‚       â”‚       â”‚       â”‚       â”œâ”€â”€ Temporal Analysis (Line plot)
â”‚       â”‚       â”‚       â”‚       â””â”€â”€ Korean Explanations (Text box)
â”‚       â”‚       â”‚       â”œâ”€â”€ _create_branch_contribution_plot()
â”‚       â”‚       â”‚       â”œâ”€â”€ _create_phoneme_attention_heatmap()
â”‚       â”‚       â”‚       â”œâ”€â”€ _create_temporal_analysis_plot()
â”‚       â”‚       â”‚       â”œâ”€â”€ _create_geometry_analysis_plot()
â”‚       â”‚       â”‚       â””â”€â”€ _add_korean_explanations()
â”‚       â”‚       â”‚
â”‚       â”‚       â””â”€â”€ hybrid_mmms_pia_explainer.py  # ğŸš§ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ (ì‘ì—… ì¤‘)
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ scripts/                    # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”‚   â”œâ”€â”€ train_pia.py                      # âœ… PIA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”‚   â”‚   â””â”€â”€ ê¸°ëŠ¥:
â”‚       â”‚   â”‚       â”œâ”€â”€ KoreanPhonemeDataset ë¡œë“œ
â”‚       â”‚   â”‚       â”œâ”€â”€ PIAModel í•™ìŠµ (CrossEntropyLoss)
â”‚       â”‚   â”‚       â”œâ”€â”€ Early stopping (patience=10)
â”‚       â”‚   â”‚       â””â”€â”€ ì¶œë ¥: outputs/pia_*/checkpoints/best.pth
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ evaluate_pia.py                   # PIA í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”‚       â”‚   â”œâ”€â”€ test_pia_from_urls.py             # URL í…ŒìŠ¤íŠ¸
â”‚       â”‚   â”œâ”€â”€ train.py                          # MMMS-BA í•™ìŠµ
â”‚       â”‚   â””â”€â”€ evaluate.py                       # MMMS-BA í‰ê°€
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ tests/                      # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚       â”‚   â”œâ”€â”€ test_phoneme_dataset.py           # Dataset í…ŒìŠ¤íŠ¸
â”‚       â”‚   â”œâ”€â”€ test_pia_alignment.py             # ìŒì†Œ ì •ë ¬ í…ŒìŠ¤íŠ¸
â”‚       â”‚   â”œâ”€â”€ test_korean_phoneme_extraction.py
â”‚       â”‚   â””â”€â”€ analyze_mismatch_phonemes.py
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ outputs/                    # ì¶œë ¥ ê²°ê³¼
â”‚       â”‚   â”œâ”€â”€ pia_aug50/                        # âœ… PIA í•™ìŠµ ê²°ê³¼ (Real ì¦ê°•)
â”‚       â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ best.pth                  # âœ… ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ (epoch 26)
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ last.pth
â”‚       â”‚   â”‚   â””â”€â”€ logs/
â”‚       â”‚   â”‚       â””â”€â”€ train_*.log
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ pia_baseline/                     # PIA ë² ì´ìŠ¤ë¼ì¸ (ì¦ê°• ì—†ìŒ)
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ korean/                           # MMMS-BA í•™ìŠµ ê²°ê³¼
â”‚       â”‚   â”‚   â””â”€â”€ evaluation/
â”‚       â”‚   â”‚       â”œâ”€â”€ xai_analysis_00000.png    # XAI ë¶„ì„ ê²°ê³¼
â”‚       â”‚   â”‚       â”œâ”€â”€ xai_analysis_00001.png
â”‚       â”‚   â”‚       â””â”€â”€ test_results.json
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ xai_comparisons/                  # âœ… XAI ë¹„êµ ë¶„ì„
â”‚       â”‚       â”œâ”€â”€ fake_sample_xai.png           # Fake ìƒ˜í”Œ XAI
â”‚       â”‚       â””â”€â”€ real_sample_xai.png           # Real ìƒ˜í”Œ XAI
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“‚ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (Analysis Scripts)
â”‚       â”‚   â”œâ”€â”€ test_pia_xai.py                   # âœ… PIA XAI í…ŒìŠ¤íŠ¸ (FAKE)
â”‚       â”‚   â”œâ”€â”€ test_pia_xai_real.py              # âœ… PIA XAI í…ŒìŠ¤íŠ¸ (REAL)
â”‚       â”‚   â”œâ”€â”€ test_pia_dataset.py
â”‚       â”‚   â”œâ”€â”€ test_pia_main_aligner.py
â”‚       â”‚   â”œâ”€â”€ analyze_phoneme_alignment.py
â”‚       â”‚   â”œâ”€â”€ analyze_phoneme_discriminability_v3.py
â”‚       â”‚   â”œâ”€â”€ analyze_phoneme_mar_overlap.py
â”‚       â”‚   â”œâ”€â”€ test_phoneme_mar_matching.py
â”‚       â”‚   â”œâ”€â”€ test_key_phonemes.py
â”‚       â”‚   â””â”€â”€ debug_phoneme_frame_matching.py
â”‚       â”‚
â”‚       â””â”€â”€ ğŸ“„ ë¬¸ì„œ (Documentation)
â”‚           â”œâ”€â”€ README.md
â”‚           â”œâ”€â”€ PIA_UNSUITABILITY_ANALYSIS_1023.md
â”‚           â”œâ”€â”€ KOREAN_PHONEME_EXTRACTION_1023.md
â”‚           â””â”€â”€ PHONEME_MATCHING_IMPLEMENTATION.md
â”‚
â””â”€â”€ ğŸ“‚ ì—°êµ¬ ì¼ì§€ (Research Diaries)
    â””â”€â”€ diary/
        â”œâ”€â”€ 2025-11-14_MAR_ìˆ˜ì •_ì—°êµ¬ë³´ê³ ì„œ.md       # âœ… MAR ì•Œê³ ë¦¬ì¦˜ ê°œì„  (v3.1â†’v3.2)
        â”œâ”€â”€ 2025-11-15_PIA_XAI_ê²°ê³¼_ë¹„êµë¶„ì„.md     # âœ… Real vs Fake XAI ë¹„êµ
        â””â”€â”€ 2025-11-15_XAI_êµ¬í˜„_ë°_ëª¨ë¸_ë¶„ì„.md     # âœ… XAI êµ¬í˜„ ìƒì„¸
```

---

## ğŸ” í•µì‹¬ íŒŒì¼ ìƒì„¸ ì„¤ëª…

### 1. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

#### `preprocess_parallel.py` (585 lines)
```python
# ë¹„ë””ì˜¤ â†’ NPZ ë³€í™˜ (PIA ì „ìš©)
def process_single_video(video_path):
    # Step 1: ìŒì†Œ ì¶”ì¶œ
    aligner = HybridPhonemeAligner(whisper_model="base", device="cuda")
    alignment = aligner.align_video(video_path)  # â†’ phonemes, intervals

    # Step 2: MAR ì¶”ì¶œ
    mar_extractor = EnhancedMARExtractor()
    geometry = mar_extractor.extract_from_video(video_path)  # â†’ (T, 1)

    # Step 3: ArcFace ì¶”ì¶œ
    arcface_extractor = ArcFaceExtractor(device="cuda", model_name="buffalo_l")
    arcface = arcface_extractor.extract_from_video(video_path)  # â†’ (T, 512)

    # Step 4: í”„ë ˆì„/ì˜¤ë””ì˜¤/ë¦½ ì¶”ì¶œ (ShortsPreprocessor)
    preprocessor = ShortsPreprocessor(config)
    result = preprocessor.process_video(video_path)

    # Step 5: NPZ ì €ì¥
    np.savez_compressed(
        output_path,
        frames=result['frames'],        # (50, 224, 224, 3)
        audio=result['audio'],          # (T_audio, 40)
        lip=result['lip'],              # (50, 96, 96, 3)
        arcface=arcface,                # (T, 512) âœ… REAL
        geometry=geometry,              # (T, 1) âœ… REAL MAR
        phoneme_labels=phoneme_labels,  # (T,) âœ… REAL
        timestamps=timestamps,          # (T,) âœ… REAL
        label=1 if label == 'fake' else 0
    )
```

**ì¶œë ¥ ìœ„ì¹˜**: `preprocessed_data_phoneme/train/00000.npz`

---

### 2. ë°ì´í„° ë¡œë”©

#### `src/data/phoneme_dataset.py` - `KoreanPhonemeDataset`
```python
def __getitem__(self, idx):
    # NPZ ë¡œë“œ
    data = np.load(npz_path)

    # ìŒì†Œ ë¼ë²¨ ì¶”ì¶œ
    phoneme_labels = data['phoneme_labels']  # (T,) - í”„ë ˆì„ë³„ ìŒì†Œ
    timestamps = data['timestamps']          # (T,) - íƒ€ì„ìŠ¤íƒ¬í”„

    # 14Ã—5 ê·¸ë¦¬ë“œ ìƒì„±
    phoneme_indices, phoneme_labels_14 = sample_phonemes_from_timestamps(
        phoneme_labels, timestamps, num_phonemes=14, frames_per_phoneme=5
    )  # â†’ (14, 5) ì¸ë±ìŠ¤

    # íŠ¹ì§• ì¶”ì¶œ
    geometry = data['geometry'][phoneme_indices]  # (14, 5, 1)
    images = data['frames'][phoneme_indices]      # (14, 5, 224, 224, 3)
    arcface = data['arcface'][phoneme_indices]    # (14, 5, 512)

    return {
        'geometry': geometry,
        'images': images,
        'arcface': arcface,
        'mask': mask,             # (14, 5) - ìœ íš¨í•œ í”„ë ˆì„
        'phonemes': phoneme_labels_14,  # List[str] - 14ê°œ ìŒì†Œ
        'label': label
    }
```

---

### 3. ëª¨ë¸ ì•„í‚¤í…ì²˜

#### `src/models/pia_model.py` - `PIAModel`
```python
class PIAModel(nn.Module):
    def __init__(self, num_phonemes=14, frames_per_phoneme=5, num_classes=2):
        # Branch 1: Geometry (MAR)
        self.geometry_branch = GeometryBranch(geo_dim=1, hidden_dim=128)

        # Branch 2: Image (ResNet + GRU)
        self.image_branch = ImageBranch(resnet_model='resnet18', hidden_dim=256)

        # Branch 3: ArcFace (Identity)
        self.arc_branch = ArcBranch(arcface_dim=512, hidden_dim=128)

        # Fusion: Cross-Attention
        self.cross_attention = CrossAttention(hidden_dim=512)

        # Classifier
        self.classifier = nn.Linear(512, num_classes)

    def forward(self, geoms, imgs, arcs, mask):
        # (B, P, F, ...) â†’ Branch outputs
        geo_out = self.geometry_branch(geoms, mask)     # (B, P, 128)
        img_out = self.image_branch(imgs, mask)         # (B, P, 256)
        arc_out = self.arc_branch(arcs, mask)           # (B, P, 128)

        # Fusion
        fused = self.cross_attention(geo_out, img_out, arc_out)  # (B, 512)

        # Classification
        logits = self.classifier(fused)  # (B, 2)

        return logits, {
            'geometry': geo_out,
            'image': img_out,
            'arcface': arc_out
        }
```

---

### 4. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

#### `scripts/train_pia.py`
```bash
# ì‚¬ìš© ì˜ˆì‹œ
python scripts/train_pia.py \
    --config configs/train_pia.yaml \
    --data-dir ../preprocessed_data_phoneme/ \
    --epochs 30 \
    --batch-size 8 \
    --augment-real \
    --augment-ratio 1.0

# ì¶œë ¥
outputs/pia_aug50/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best.pth      # ìµœê³  ì„±ëŠ¥ (epoch 26, val_acc=...)
â”‚   â””â”€â”€ last.pth
â””â”€â”€ logs/
    â””â”€â”€ train_20251115_*.log
```

---

### 5. XAI ë¶„ì„

#### `src/xai/pia_explainer.py` - `PIAExplainer`
```python
def explain(self, geoms, imgs, arcs, mask, phonemes, timestamps):
    # 1. Forward pass
    logits, branch_outputs = self.model(geoms, imgs, arcs, mask)
    prediction = 'FAKE' if logits[0, 1] > logits[0, 0] else 'REAL'
    confidence = torch.softmax(logits, dim=1)[0, 1].item()

    # 2. Branch Contribution Analysis
    branch_contributions = self._compute_branch_contributions(branch_outputs)
    # â†’ {'geometry': 15.41%, 'image': 78.33%, 'arcface': 6.26%}

    # 3. Phoneme Attention Analysis
    phoneme_attention = self._compute_phoneme_attention(branch_outputs, mask)
    # â†’ (14, 5) attention weights

    # 4. Temporal Pattern Analysis
    temporal_analysis = self._analyze_temporal_patterns(
        branch_outputs, timestamps
    )

    # 5. Geometry Anomaly Analysis
    geometry_analysis = self._analyze_geometry_anomalies(
        geoms, phonemes, timestamps
    )

    # 6. Korean Explanation Generation
    korean_summary = self._generate_korean_explanation(
        prediction, confidence, branch_contributions,
        phoneme_attention, geometry_analysis
    )

    return {
        'prediction': prediction,
        'confidence': confidence,
        'branch_contributions': branch_contributions,
        'top_branch': max(branch_contributions, key=branch_contributions.get),
        'phoneme_attention': phoneme_attention,
        'temporal_analysis': temporal_analysis,
        'geometry_analysis': geometry_analysis,
        'korean_summary': korean_summary
    }
```

---

#### `src/xai/pia_visualizer.py` - `PIAVisualizer`
```python
def visualize_full_analysis(self, xai_result, output_path, video_id):
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # Subplot 1: Branch Contribution
    self._create_branch_contribution_plot(axes[0, 0], xai_result)

    # Subplot 2: Phoneme Attention Heatmap
    self._create_phoneme_attention_heatmap(axes[0, 1], xai_result)

    # Subplot 3: Temporal Analysis
    self._create_temporal_analysis_plot(axes[1, 0], xai_result)

    # Subplot 4: Korean Explanations
    self._add_korean_explanations(axes[1, 1], xai_result)

    plt.suptitle(f"PIA XAI Analysis - {video_id}", fontsize=16)
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
```

**ì¶œë ¥ ì˜ˆì‹œ**: `outputs/xai_comparisons/fake_sample_xai.png`

---

### 6. XAI í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

#### `test_pia_xai.py` (FAKE ìƒ˜í”Œ)
```bash
python test_pia_xai.py

# ì¶œë ¥
[PIA XAI Analysis - FAKE Sample]
Prediction: FAKE (100% confidence)
Top Branch: image (78.33%)
Top Phoneme: ã…Š (51.86%)

Saved: outputs/xai_comparisons/fake_sample_xai.png
```

#### `test_pia_xai_real.py` (REAL ìƒ˜í”Œ)
```bash
python test_pia_xai_real.py

# ì¶œë ¥
[PIA XAI Analysis - REAL Sample]
Prediction: REAL (100% confidence)
Top Branch: image (84.55%)
Top Phoneme: ã… (98.74%)

Saved: outputs/xai_comparisons/real_sample_xai.png
```

---

## ğŸ“Š PIA XAI ë¶„ì„ ê²°ê³¼ ìš”ì•½ (2025-11-15 ì—°êµ¬)

### Real vs Fake ë¹„êµ

| íŠ¹ì§• | FAKE ì˜ìƒ | REAL ì˜ìƒ |
|------|-----------|-----------|
| **Prediction** | FAKE (100%) | REAL (100%) |
| **Top Phoneme** | ã…Š (51.86%) | ã… (98.74%) |
| **Attention ë¶„í¬** | ë‹¤ì¤‘ ë¶„ì‚° | ë‹¨ì¼ ì§‘ì¤‘ |
| **Visual ê¸°ì—¬ë„** | 78.33% | 84.55% |
| **Geometry ê¸°ì—¬ë„** | 15.41% | 13.16% |
| **ArcFace ê¸°ì—¬ë„** | 6.26% | 2.29% |
| **MAR í‰ê· ** | 0.059 | 0.017 |
| **MAR ìµœëŒ€** | 0.322 | 0.599 |

### í•µì‹¬ ë°œê²¬

1. âœ… **Visual Branch ì§€ë°°ì„±**: 78-85% ê¸°ì—¬ë„ (ì… ëª¨ì–‘ì´ í•µì‹¬)
2. âœ… **Real vs Fake ìŒì†Œ íŒ¨í„´**: ã… (ìì—°) vs ã…Š (ì´ìƒ)
3. âœ… **Attention ë¶„í¬ ì°¨ì´**: Realì€ ì§‘ì¤‘, FakeëŠ” ë¶„ì‚°
4. âš ï¸ **MAR ë‚®ìŒ**: í•œêµ­ì–´ íŠ¹ì„± ë°˜ì˜ í•„ìš”

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ (Full Pipeline)

### 1ë‹¨ê³„: ì „ì²˜ë¦¬
```bash
cd E:\capstone

# í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬
python preprocess_parallel.py --split train --workers 6
# ì¶œë ¥: preprocessed_data_phoneme/train/00000.npz, ...
```

### 2ë‹¨ê³„: í•™ìŠµ
```bash
cd mobile_deepfake_detector

# PIA ëª¨ë¸ í•™ìŠµ
python scripts/train_pia.py \
    --config configs/train_pia.yaml \
    --data-dir ../preprocessed_data_phoneme/ \
    --epochs 30 \
    --augment-real \
    --augment-ratio 1.0

# ì¶œë ¥: outputs/pia_aug50/checkpoints/best.pth
```

### 3ë‹¨ê³„: XAI í…ŒìŠ¤íŠ¸
```bash
# Fake ìƒ˜í”Œ XAI ë¶„ì„
python test_pia_xai.py
# ì¶œë ¥: outputs/xai_comparisons/fake_sample_xai.png

# Real ìƒ˜í”Œ XAI ë¶„ì„
python test_pia_xai_real.py
# ì¶œë ¥: outputs/xai_comparisons/real_sample_xai.png
```

---

## âš™ï¸ í•µì‹¬ ì„¤ì •

### `configs/train_pia.yaml`
```yaml
data:
  num_phonemes: 14              # í•œêµ­ì–´ í•µì‹¬ ìŒì†Œ ê°œìˆ˜
  frames_per_phoneme: 5         # ìŒì†Œë‹¹ í”„ë ˆì„ ê°œìˆ˜
  data_dir: ../preprocessed_data_phoneme/

model:
  arcface_dim: 512              # ArcFace ì„ë² ë”© ì°¨ì›
  geo_dim: 1                    # MAR (Mouth Aspect Ratio)
  embed_dim: 512                # Fusion ì„ë² ë”© ì°¨ì›
  num_heads: 8                  # Cross-Attention í—¤ë“œ ìˆ˜
  num_classes: 2                # Real/Fake
  use_temporal_loss: false

training:
  batch_size: 8
  learning_rate: 0.0001
  num_epochs: 30
  early_stopping_patience: 10

output:
  checkpoint_dir: outputs/pia_aug50/checkpoints/
  log_dir: outputs/pia_aug50/logs/
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

1. **MAR ì•Œê³ ë¦¬ì¦˜ ê°œì„ **: `diary/2025-11-14_MAR_ìˆ˜ì •_ì—°êµ¬ë³´ê³ ì„œ.md`
   - Inner lip â†’ Outer lip ë³€ê²½
   - Mouth-box â†’ Face-height ì •ê·œí™”
   - MAR ê°’ 17ë°° ì¦ê°€ (0.03 â†’ 0.51)

2. **PIA XAI ë¹„êµ ë¶„ì„**: `diary/2025-11-15_PIA_XAI_ê²°ê³¼_ë¹„êµë¶„ì„.md`
   - Real vs Fake attention íŒ¨í„´
   - Visual Branch ì§€ë°°ì„± (78-85%)
   - ìŒì†Œë³„ attention ë¶„í¬

3. **XAI êµ¬í˜„ ìƒì„¸**: `diary/2025-11-15_XAI_êµ¬í˜„_ë°_ëª¨ë¸_ë¶„ì„.md`
   - Branch contribution ê³„ì‚°
   - Phoneme attention ë¶„ì„
   - Korean explanation ìƒì„±

---

## ğŸ”§ ìŒì†Œ ì¶”ì¶œê¸° ë³€ì²œì‚¬

| ë²„ì „ | íŒŒì¼ëª… | ìƒíƒœ | ë°©ì‹ |
|------|--------|------|------|
| v1 | `hybrid_phoneme_aligner.py` | âš ï¸ êµ¬ë²„ì „ | WhisperX + Wav2Vec2 |
| v2 | `hybrid_phoneme_aligner_v2.py` | âœ… **í˜„ì¬ ì‚¬ìš©** | WhisperX + Wav2Vec2 + ìëª¨ ë¶„ë°° |
| v3 | `hybrid_phoneme_aligner_v3_failed.py` | âŒ ì‹¤íŒ¨ | ì‹¤í—˜ ë²„ì „ |
| - | `wav2vec2_korean_phoneme_aligner.py` | âš ï¸ ì‚¬ìš© ì•ˆ í•¨ | Wav2Vec2 ë‹¨ë… |
| - | `pia_main_phoneme_aligner.py` | âš ï¸ ë³´ì¡° | PIA-main ì›ë³¸ ìŠ¤íƒ€ì¼ |

**ê²°ë¡ **: `hybrid_phoneme_aligner_v2.py`ê°€ ì‹¤ì œ ì „ì²˜ë¦¬ì— ì‚¬ìš©ë¨!

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-17
**ê²€ì¦ ì™„ë£Œ**: PIA ì „ì²˜ë¦¬ â†’ í•™ìŠµ â†’ XAI ì „ì²´ íŒŒì´í”„ë¼ì¸